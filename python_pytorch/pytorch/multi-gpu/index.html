<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="lipingcoding">
        
        <link rel="shortcut icon" href="../../../img/favicon.ico">
        <title>Multi gpu - Lipingcoding</title>
        <link href="../../../css/bootstrap.min.css" rel="stylesheet">
        <link href="../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/darcula.min.css">

        <script src="../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../js/bootstrap.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="../../..">Lipingcoding</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="navitem">
                                <a href="../../.." class="nav-link">Home</a>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Research <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../../research/gnn/overview/" class="dropdown-item">GNN</a>
</li>
                                    
<li>
    <a href="../../../research/recsys/overview/" class="dropdown-item">RecSys</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown">Programming <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li>
    <a href="../../../programming/python/overview/" class="dropdown-item">python</a>
</li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a href="https://github.com/lipingcoding/blog/edit/master/docs/python_pytorch/pytorch/multi-gpu.md" class="nav-link"><i class="fa fa-github"></i> Edit on GitHub</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-level="1"><a href="#_1" class="nav-link">基本使用</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#gpu" class="nav-link">指定 gpu</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#_3" class="nav-link">最佳实践</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
            
            <li class="nav-item" data-level="1"><a href="#_5" class="nav-link">模型的保存和加载</a>
              <ul class="nav flex-column">
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<p>只要你需要在 gpu 上运行 pytorch, 你就(至少)需要解决以下若干问题:</p>
<ul>
<li>数据如何送到 gpu 上</li>
<li>模型(的参数)如何放到 gpu 上</li>
<li>模型的参数值如何 save/load 以方便 resume_train 或者 eval</li>
</ul>
<p>本文将提供关于 pytorch (多) gpu 运行, 你所需要知道的一切!</p>
<h1 id="_1">基本使用</h1>
<p>单个 gpu 运行是多个 gpu 运行的特例, 所以我们主要介绍多个 gpu 如何使用.</p>
<p>为了在多个 gpu 上运行, 首先, 数据(包括样本和标签)必须在 gpu 上, 模型也必须在 gpu 上. 按理说, 要想在多个 gpu 上跑, 应该把数据和模型都拷贝到多个 gpu 上, 但其实不用. 只需要将它们放到一张 gpu 上就行.</p>
<p>以一个 train.py 的框架为例来说明</p>
<pre><code>import torch
import argparse

parser = argparse.ArgumentParser()
parser.add_argument('--cuda', default=True, type=bool)
args = parser.parse_args()


&quot;&quot;&quot;
通过设置默认 Tensor 类型, 可以使得数据的 Tensor 都在 cuda:0 上
此步可以封装成一个函数 
&quot;&quot;&quot;
if torch.cuda.is_available():
    if args.cuda:
        torch.set_default_tensor_type('torch.cuda.FloatTensor')
    if not args.cuda:
        print(&quot;WARNING: It looks like you have a CUDA device, but aren't &quot; +
              &quot;using CUDA.\nRun with --cuda for optimal training speed.&quot;)
        torch.set_default_tensor_type('torch.FloatTensor')
else:
    torch.set_default_tensor_type('torch.FloatTensor')



def train():
    &quot;&quot;&quot;
    下面假设你有了一个模型, 并且初始化了一个 net
    你只需要在 定义 optim 前, 进行如下
    &quot;&quot;&quot; 
    net = build_net(..., phase = 'train', ...)
    net = torch.nn.DataParallel(net) 
    net = net.cuda() 

    optimizer = optim.SGD(net.parameters(), lr=args.lr, ... )

    ...

if __name__ == '__main__':
    train()
</code></pre>
<p>重点解释如下两行代码:</p>
<pre><code>net = torch.nn.DataParallel(net) 
net = net.cuda()
</code></pre>
<h4 id="dataparallel">DataParallel</h4>
<p>DataParallel 是一个 wrapper, DataParallel 字面上是并行(处理)数据, 它告诉 pytorch : 等会儿在进行模型运行的时候, 需要将 net 并行地 apply 到 data 上面.</p>
<h3 id="cuda">.cuda()</h3>
<p>.cuda 这个语句我们经常看到, 它是什么意思呢? 就是把对象拷贝到 gpu 上, 由于没有提供具体的 device id, 它会拷贝到默认的 cuda device 上, 也就是 cuda:0 上. 所以 net = net.cuda( ) 就是将网络布置到 cuda: 0 上.</p>
<h4 id="_2">具体运行</h4>
<p>现在我们可以考察模型到底是怎么运行的了. 前面我们已经将数据和模型拷贝到 cuda:0 上了, cuda:0 也就是它们的原始 device. 由于前面申明了 DataParallel, pytorch 会在多个 gpu (稍后会介绍如何指定)上并行地将 net apply 到 data 上. 为了达到这个目的, 需要完成如下步骤:</p>
<ol>
<li>将数据从 cuda:0 上拷贝到多个 gpu 上</li>
<li>将模型以及参数从 cuda:0 上拷贝到多个 gpu 上</li>
<li>在多个 gpu 上进行 forward, 并计算 loss</li>
<li>将 loss 汇总到 cuda:0 上</li>
<li>在 cuda:0 上进行 backward</li>
</ol>
<h2 id="gpu">指定 gpu</h2>
<p>前文中, 我们一直都没有涉及具体指定 gpu, 现在, 我们就来仔细考察这个问题.</p>
<h4 id="cuda_visible_devices">CUDA_VISIBLE_DEVICES</h4>
<p>首先, 我们需要知道 pytorch 是如何知道电脑上的 gpu 的. 系统有个变量 <code>CUDA_VISIBLE_DEVICES</code> , 它指明了哪些 gpu 是 pytorch 是可以<strong>看见</strong>的.</p>
<p>比如, 你有 4 张 gpu, 编号为 0~3, 如果你 <code>CUDA_VISIBLE_DEVICES = 1, 3</code>, 那么 pytorch 会认为你只有两张 gpu, 它会认为 gpu 1 是 cuda:0, gpu 3 是 cuda:1. 所以 cuda:0, 不一定是 gpu: 0.</p>
<h4 id="cuda-device">默认 cuda device</h4>
<p><code>xx.cuda()</code> 如果不指定 device 时, 就会将 <code>xx</code> 拷贝到默认的 cuda device , 也就是 cuda:0 (不一定是你的第一张 gpu).</p>
<h4 id="torchnndataparallel">torch.nn.DataParallel()</h4>
<p>其实 <code>DataParallel</code> 这个函数还有两个可选参数 <code>device_ids, output_device</code>, 前者是指在哪些 gpu 上运行, 默认是全部可见 gpu 上, 后者是指最终将 loss 汇总到哪个 gpu 上, 默认是 cuda:0 .</p>
<h2 id="_3">最佳实践</h2>
<p>根据我的经验, 最好的使用方法就是:</p>
<ol>
<li>利用 torch.set_default_tensor_type 设置默认 Tensor, 这样, 数据会在 cuda:0 上</li>
<li>利用 torch.nn.DataParallel 声明</li>
<li>利用 net = net.cuda( ), 将模型布置到 cuda:0 上</li>
<li>在命令行调用时, 形如 <code>CUDA_VISIBLE_DEVICES = 2,3 python train.py</code> , 在程序外部指定具体 gpu</li>
</ol>
<p>还有一些细节可以考察, 比如:</p>
<ul>
<li>如果最开始模型和数据不在同一个 gpu 上会怎样</li>
<li>如果最开始模型和数据所在的 gpu 不在 DataParallel 中的 device_ids 中会怎样</li>
</ul>
<h3 id="_4">注意:</h3>
<p>多 gpu 运行的过程导致一些问题:</p>
<ul>
<li>在 forward 的过程中, 对于网络中参数的更改是无效的,因为 forward 结束后, gpu 上的网络就会被清除.</li>
<li>会导致 gpu 内存使用出现 imbalanced 的情况</li>
<li>反复拷贝模型参数</li>
</ul>
<h1 id="_5">模型的保存和加载</h1>
<p>在训练的过程中, 会保存模型当前状态, 也就是 stat_dict, 将模型中的参数, 以字典的形式序列化到硬盘上. 涉及到 gpu 的时候, stat_dict 的保存和加载会比较复杂, 可以参考 <a href="https://pytorch.org/tutorials/beginner/saving_loading_models.html?highlight=load_state_dict">官方 doc</a>.</p>
<p>为什么模型的保存和加载会有这些门道呢? 原因很简单, stat_dict 序列化后保存在硬盘上, 反序列化是在 cpu 上进行的. 加载时, 希望加载到 gpu 上, 所有这一步就需要用户定义.</p>
<p>不过, 经过 DataParallel 包装后的模型, 会很简单, 可以在任何地方随意加载, <a href="https://pytorch.org/tutorials/beginner/saving_loading_models.html?highlight=load_state_dict">官方 doc</a> 也有描述. 但是这地方有个坑, 我们通过一个 demo 来说明</p>
<p>先定义一个 模型</p>
<pre><code>import torch
from torch import nn

class MyModule(nn.Module):
    def __init__(self, input_size, output_size):
        super(MyModule, self).__init__()
        self.input_size, self.output_size = input_size, output_size
        self.fc = nn.Linear(self.input_size, self.output_size)

    def forward(self, x): 
        return self.fc(x)
In [14]: net = MyModule(3,5)                                                                                                                           

In [15]: torch.save(net.state_dict(), 'net_parm.pth')                                                                                                  

In [16]: for k in torch.load('net_parm.pth'): 
    ...:     print(k) 
    ...:                                                                                                                                               
fc.weight
fc.bias


In [19]: paral_net = torch.nn.DataParallel(net)                                                                                                        

In [20]: torch.save(paral_net.state_dict(), 'paral_net_parm.pth')

In [24]: for k in torch.load('paral_net_parm.pth'): 
    ...:     print(k) 
    ...:                                                                                                                                               
module.fc.weight
module.fc.bias
</code></pre>
<p>可以发现, 经过 DataParallel 包装后的网络, 序列化后, state_dict 的键值 多了一个 <code>.module</code>, 如果这个序列化文件之后被一个未经DataParallel 包装的网络 load 后, 就会报错.</p>
<pre><code>In [25]: new_net = MyModule(3,5)                                                                                                                       

In [26]: new_net.load_state_dict(torch.load('paral_net_parm.pth'))                                                                                     


RuntimeError: Error(s) in loading state_dict for MyModule:
    Missing key(s) in state_dict: &quot;fc.weight&quot;, &quot;fc.bias&quot;. 
    Unexpected key(s) in state_dict: &quot;module.fc.weight&quot;, &quot;module.fc.bias&quot;.
</code></pre>
<p>为什么会这样呢?</p>
<pre><code>In [27]: net = MyModule(3,5)                                                                                                                           

In [28]: paral_net = torch.nn.DataParallel(net)                                                                                                        

In [29]: torch.save(paral_net.module.state_dict(),  'paral_net_parm.pth')         # 多了一个 .module                                                                      

In [30]: for k in torch.load('paral_net_parm.pth'):  
    ...:     print(k) 
    ...:                                                                                                                                               
fc.weight
fc.bias

In [32]: paral_net.module.load_state_dict(torch.load('paral_net_parm.pth'))    # 也多了一个 .module
</code></pre>
<p>通过代码可以发现, DataParallel 相当于将原来的 net 包装了一个 module, 所以对于 DataParallel 的模型, 有两种层次的 save/load, 只要相互匹配即可!</p></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../js/base.js" defer></script>
        <script src="../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
