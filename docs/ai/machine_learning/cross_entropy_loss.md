深度学习中, 经常看到如下名词, KL散度, cross entropy (交叉熵), softmax, softmax loss 等, 到底是个什么东西, 相互之间又有什么关系, 本文一一解答.





## softmax

这个比较简单, 多用于分类网络的最后一层, 用于输出属于各类的概率.
$$
softmax(x)_i = \frac{\exp(x_i)}{\sum_j \exp(x_j)}
$$
Softmax 输入向量的维度和输出向量的长度是相同的, 而且输出向量的各项非负, 且和为 1, 因而可以理解为概率.

## softmax loss

就是 cross entropy, 不建议使用这个名称

## KL 散度

信息论中又称为相对熵或者 KL 距离, 用来衡量两个分布的距离. 但其实, KL 散度并不是严格的距离, 因为不满足对称性和三角不等式.
$$
\begin{align}
D_{KL}(p,q) &=& E_p\log{\frac{p}{q}}\
&=& -E_p\log(q) - H(p)
\end{align}
$$

## cross entropy

就是大名鼎鼎的交叉熵了, 机器学习里面通常用
$$H(p,q) = -E_p\log(q)$$
来表示, 但在信息论中, $H(p,q)$ 代表联合熵. 之前说过, KL 散度可以衡量两个分布的距离, 那么交叉熵为什么也能衡量两个分布的距离呢? 因为分布 p 是标签决定的, 所以 $D_{KL}(p,q)$ 中的 $H(p)$ 是定值, 我们的任务是寻找一个分布q, 作为对分布 p 的估计, 所以 q 是变化量, 所以 $D_{KL}(p,q)$ 完全由交叉熵 $-E_p\log(q)$决定.